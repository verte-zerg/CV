# Mikhail Zhuk
**Position**: Senior Python Developer

**Other formats**:
[**PDF**](http://michail-zhuk-cv.s3-website.us-east-2.amazonaws.com/),
[**Markdown**](https://github.com/verte-zerg/CV/blob/gh-pages/index.md),
[**Webpage**](https://verte-zerg.github.io/CV/)

# Contact Info
- [**Telegram**](https://t.me/verte_zerg)
- [**Linkedin**](https://www.linkedin.com/in/mikhail-zhuk-036a061a1/)
- [**Email**](mailto:michail.zhuk@protonmail.com)

# Professional profile
- Strong development skills in Python.
- Hand-on practice with developing, building and running WEB-services using Aiohttp, FastAPI, Flask.
- Skilled in working with Docker, Kubernetes, GitLab CI/CD, Flux, Helm.
- Good understanding of Systems Design.
- Proficient in working with SQL queries and SQL Databases.
- Skilled in working with Linux systems.
- Good experience of Hadoop ecosystem.
- Care about quality and clean code.
- Agile and scrum methodologies experience.
- Personal skills: quick-learner, high analytic and social skills, communicative, good team player, responsible and hard-working.

# Skills summary
- **Programming Languages**: Python, Bash, SQL.
- **Infrastructure**: Docker, Kubernetes, GitLab CI/CD, Flux, Helm.
- **Data Engineering**: PySpark, Airflow, Hadoop stack.
- **Storages**: S3, PostgreSQL, HBase, Elasticsearch.
- **Cloud Technologies**: AWS, IBM Cloud.
- **Foreign Languages**: English - B1.

# Professional experience

## ChatBot Service - Senior Python Developer / Technical Leader

ChatBot Service helps users to find answers to most typical questions, it can answer to 80% of these questuions. Otherwise the questions are redirected to support center. Daily chatbot handles 20k dialogs.

### Responsibilities
- Design system architecture.
- Develop new core features.
- Release new chatbot versions.
- Integration ML model in service.
- Optimization and refactoring of existing code.
- Writing unit and integration tests.
- Mentoring coulegues.
- Code review.
- Comminication with business.
- Organize process workflow: transfer bussiness needs to technical tasks and estimation tasks.

### Stack
AioHTTP, Minio, PostgreSQL, Kubernetes, Helm, Flux, GitLab CI/CD, Prometheus, Jaeger, Pytest, MyPy, Flake.

## Solution for a proactive client support - Software Engineer

Cloud solution to collect, analyze, predict client's issues proactively. Large volumes of data are collected and processed in real-time an store in Elasticsearch. Is further analyzed using Kibana dashboards. 

### Responsibilities
- Elasticsearch cluster provisioning automation with Terraform.
- Administration of 15-node Elasticsearch clusters.
- Setting up automated monitoring of Elasticsearch Cluster with IBM Cloud Functions and Python.
- Analysis, developing queries to Elasticsearch.
- Developing Python script to automate deployment. 
- Automated report generation and distribution by Email using IBM Cloud Functions and Python.
- Setting up Slack notifications for daily tasks and data inconsistencies.
- Building visualizations and dashboards in Kibana.
- Developing NiFi jobs to consume data from Kafka, transform it and ingest into Elasticsearch;

### Stack
Elasticsearch, Kibana, Python, IBM Cloud (Functions, Cloud Foundry), Kafka, Apache NiFi, Docker, GitHub.

## Fleet Online Solution - Big Data Developer

The goal of the project is to provide daily data for a service that allows businesses operating a fleet of vehicles to get quick resolution to any tire issue in their fleet.

### Responsibilities
- Design new pipelines.
- Collecting data from various sources and processing it.
- Monitor existing pipelines in test and production.
- Cover code with test cases.
- Writing documentation.
- Analyze data using AWS Athena.
- Performance optimization.

### Stack
Pyspark, AWS EMR, AWS Lambda, AWS Athena, S3, Airflow, Pytest, MyPy.

## Bank Recommendation System - Big Data Developer

Constructing real-time multi-channel recommendation system for a bank with 700k customers.

### Responsibilities
- Using NiFi for building event pipelines, integrating it with message brokers.
- Performing stress testing, preparing reports (75 EPS during peak hours, 95th percentile is 4 seconds).
- Writing Pyspark scripts for daily data ingestion (Terraform to HBase), transformation, aggregation.
- Setting up Hadoop cluster using Cloudera, automating deployments for Nifi flows, HBase tables, Lily/Solr schemes.
- Developing Python script to automate deployment. 
- Indexing data for fast search using Solr.
- Preparation of data for reports using Hive.

### Stack
Hadoop (HDFS, Hive, Solr, HBase), NiFi, Pyspark, Kafka, IBM MQ, Terradata.

# Other activities
Teacher - IBA Students Lab in cooperation with universities (BSU and BSUIR).

## Data Engineering Lab - Teacher 

Teaching students, providing offline and online lectures, supervision of course projects, technical inverviewing of candidates.

### Course program
- Database design concepts.
- SQL (basic and advanced operations).
- Python: (basics, advanced concepts, Flask, Swagger, Unit testing).
- Hadoop ecosystem (HDFS, MapReduce, YARN, Hive, HBase).

# Education
**Belarusian State University**: Computational physics (physicist-programmer) 2015-2020 
